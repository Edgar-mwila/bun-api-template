# Written by David Wallach
# Copyright of Stocker.io 2017
# This is the main calling function 
# to run our algorithm 
import urllib2
from bs4 import BeautifulSoup
import re
import csv
import numpy as np
import getPerception
import webbrowser 

global queries 
queries = []

global depth
depth = 0

MAX_DEPTH = 3


def build_query():
	global queries #make reference to global queries 
	tickers = ["gopro"]
	news_sources = ["marketwatch"]
	extra_params = ["news"]
	# extra_params = ["news", "markets"]
	# tickers = ["ua", "yhoo", "google", "gpro", "aapl"]
	# news_sources = ["seekingalpha","bloomberg","marketwatch"]
	# query=raw_input("enter your query:")

	for i,ticks in enumerate(tickers):
		for j,sources in enumerate(news_sources):
			for k,params in enumerate(extra_params):
				queries.append([tickers[i], news_sources[j], extra_params[k]])

	print("****************************")
	print("QUEREIS (" + str(len(queries)) +") : ")
	print("****************************")

	for i in queries:
		print(i[0] + " " + i[1] + " " + i[2])

#
#
#PRINT ALL THE TITLE TAGS
#MAKE EXTERNAL CALLS TO PARSE INFORMATION AND 
#GET INFO
#
def get_info(links, source, company, depth):

	for link in links:
		url = link.rstrip().replace("u'","")
		url = link.rstrip().replace("'","")
		req = urllib2.Request(url, headers={'User-Agent': 'Mozilla/5.0'})

		if(url[0:4] != "http"):
			continue
		try:
			page = urllib2.urlopen(req)
			soup = BeautifulSoup(page.read(), "html.parser") #get all html info from webpage
			article_title = soup.find_all("title") #used for printing title to user

			article_date = soup.find_all("date")
			if len(article_date) is 0:
				article_date = soup.find_all("time")
				try:
					article_date = soup.time.attrs['datetime']
				# except AttributeError:
				# 	article_date = "NULL"
				except KeyError:
					print "\nKey error from datetime"
					try: 
						article_date = soup.find_all(id="published-timestamp")
						article_date = re.findall(r'<span>(.*?)</',str(article_date),re.DOTALL)[0]
					except AttributeError:
						print "\nAttributeError\n"
						article_date = "NULL"
					except TypeError:
						article_date = "NULL"
					except KeyError:
						print "\nKeyError\n"
						article_date = "NULL"
				except AttributeError:
					print "\nAttribute error from datetime"
					try: 
						print "\ntrying published-timestamp"
						article_date = soup.find_all(id="published-timestamp")
						article_date = re.findall(r'<span>(.*?)</',str(article_date),re.DOTALL)[0]
					except AttributeError:
						print "\nAttribute error from published-timestamp\n"
						article_date = "NULL"
					except TypeError:
						print "\nType error from published-timestamp\n"
						article_date = "NULL"
					except KeyError:
						print "\nKey error from published-timestamp\n"
						article_date = "NULL"
					except IndexError:
						print "\nIndex error from published-timestamp"
						article_date = "NULL"
				except TypeError:
					print "\nType error from datetime"
					article_date = "NULL"



			# else:
			# 	article_date = soup.find_all("date")
			article_title = make_title_pretty(article_title)
			print "\n\nScraping web for company : " + company
			print "Currently parsing article : " + article_title + "\n" + "From source : "+ source 
			print "URL is : " + url
			print "Date posted : " + str(article_date)
			print "HTTP Status of Request : " + str(page.getcode()) + "\n"

			suburls = getPerception.parser(str(soup), url, company, source) #do the heavy lifting of breaking up the strings for the word counter
		
		except urllib2.HTTPError, e:
			print "Uh oh there was an HTTP error with opening this url: \n" + str(url)+ "\n" + "Error code " + str(e.code) + "\n" 
			continue
		except urllib2.URLError, e:
			print "Uh oh there was an error with the URL :"
			print "Error arguments" + str(e.args)
			continue

		# if depth < MAX_DEPTH:
			# get_info(suburls, source, company, depth+1)
		# print suburls
#
#
#REMOVE THE HTML FROM THE TITLE 
#
def make_title_pretty(title_html):
	title = re.findall(r'>(.*?)<',str(title_html),re.DOTALL)
	return title[0]

#
#
#QUERY GOOGLE TO GET URLS 
#
#
def web_scraper():
	#build the queries 
	#think about changing "news" to be a var
	#and make the top function a 3 dimensional array so we can search
	#news, markets, up, down, ect. 
	i = 0
	for i in range(0,len(queries)):

		query = queries[i][0] + " " + queries[i][1] + " " + queries[i][2]
		query=query.strip().split()
		query="+".join(query)

		#html request and Beautiful Soup parser 
		html = "https://www.google.co.in/search?site=&source=hp&q="+query+"&gws_rd=ssl"
		req = urllib2.Request(html, headers={'User-Agent': 'Mozilla/5.0'})
		soup = BeautifulSoup(urllib2.urlopen(req).read(),"html.parser")

		#Re to find URLS
		reg=re.compile(".*&sa=")

		#get all web urls from google search result 
		links = []
		for item in soup.find_all('h3', attrs={'class' : 'r'}):
		    line = (reg.match(item.a['href'][7:]).group())
		    links.append(line[:-4])

		#ADD SPACING
		j = 0
		for j in range(0,3):
			print('**********************************\n')

		print "URLs to parse:"
		print('--------------------------------------')
		for link in links:
			url = link.rstrip().replace("u'","")
			url = link.rstrip().replace("'","")
			print url
		print('--------------------------------------')
		get_info(links, queries[i][1], queries[i][0], 0)
		i += 1

#
#RESET DEPTHS FOR SUBLINK TRAVERSALS
#
#
def reset_depth():
	global depth
	depth = 0


if __name__ == "__main__":
	reset_depth()
	build_query() #build the google search
	web_scraper() #get the raw data from the query to be passed into get_info()
	# getPerception.printer() #print all the words and sentences generated from the query
	getPerception.call_word_counter() #generate tuples of words and their count to export to .csv file



