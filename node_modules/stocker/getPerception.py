# Written by David Wallach
# Copyright of Stocker.io 2017
# This uses our statistical models to 
# rate articles on which we base our investing decisions
import re
import wordCounter
import string
from nltk.tokenize  import regexp_tokenize
import nltk.data
import json


#declare general variables 
word_counter = False

#declare globals 
global sentences
sentences = []

global words 
words = []


#
#
#PARSE HTML TO REMOVE GARBAGE 
#AND SUBLINKS
#
#
def parser(html, url, company, source):
	pertinent_info = []
	sublinks = []
	pattern = re.compile(r'<p>.*?</p>') #find only the ptag 
	data = pattern.findall(html)
	pertinent_info = pertinent_info + data
	clean_html = ""
	replace_dict = {'<p>': '', '</p>': '', 
					'<strong>': '', '</strong>': '', 
					'<a>': '','</a>': '',
					'<em>': '', '</em>': '',
					'<span>': '', '</span>': ''
					} #html tags to remove

	for info in pertinent_info:
		robj = re.compile('|'.join(replace_dict.keys())) 
		clean_html = robj.sub(lambda m: replace_dict[m.group(0)], info) #removing the html tags e.g. <p> and </p>
		sublinks_local = re.findall(r'<a.*?>', clean_html, re.DOTALL) #get all sublinks from inside paragraphs of HTML
		clean_html = re.sub(r'<a.*?>', ' ',clean_html) #remove a href links

		#get rid of all extraneous chars 
		clean_html = re.sub('[^A-Za-z0-9]+', ' ', clean_html, re.DOTALL)

		for link in sublinks_local:
			sublinks.append(link)

		#send clean HTML to sentence and word parser
		sentences_to_json = sentence_parser(clean_html)
		words_to_json = word_parser(clean_html)

		#
		# WRITE DATA TO JSON
		#
		data = {
			"company" : company,
			"source" : source,
			"url" : url,
			"sentences" : sentences_to_json,
			"words" : words_to_json
		}

		# jsonData = json.dumps(data)
		# print(jsonData)

		with open('data.json') as feedsjson:
			feeds = json.load(feedsjson)

		feeds.append(data)
		with open('data.json', mode='w') as f:
			feeds.append(data)
			f.write(json.dumps(feeds, indent=2))

	return sublinks




#
#
#PARSE HTML TO PUT IT IN MORE READABLE FORMAT OF WORDS TO APPLY
#PATTERN MATCHING TO
#
def word_parser(html):
	global word_counter_input_str #estalish reference to global var 
	global words
	html = html.decode('utf-8','ignore')
	words_temp = nltk.tokenize.word_tokenize(html)
	for word in words_temp:
		words.append(word)
	return words_temp

#
#
#PARSE HTML TO PUT IT IN MORE READABLE FORMAT OF SENTENCES TO APPLY
#PATTERN MATCHING TO
#
def sentence_parser(html):
	global sentences
	sentences_local = regexp_tokenize(html, pattern=r'\.(\s+|$)', gaps=True)
	for sentence in sentences_local:
			sentences.append(sentence)
	return sentences_local


#
#
#ONCE WE HAVE LOOPED THROUGH ALL OF OUR ARTICLES WE WANT TO CALL THE WORD COUNTER
#AND ADD ALL THE INFORMATION INTO THE .CSV FILE
#
def call_word_counter():
	if word_counter:
		wordCounter.count_words(words)


#
#
#PRINT THE ARRAYS OF WORDS AND SENTENCES 
#
#
def printer():
	print words
	print "/////////////////////////////"
	for sentence in sentences:
		print sentence + "\n"

